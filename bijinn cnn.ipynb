{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(867, 3, 1024)\n",
      "(867, 14)\n",
      "(867, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#cnn\n",
    "\"\"\"data_load関数\n",
    "   X_train,y_train=load_data('train')\n",
    "   X_test,y_test=load_data('test)\n",
    "   cnn:tensorflow layersAPiで実装\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "label_dict={'aoyama':0,'komazawa':1,'meigaku':2,'misaki':3,'obirin':4,'phoenix':5,'saidai':6\n",
    "           ,'sekei':7,'senshu':8,'teikyo':9,'tokyo':10,'twu':11,'ycu':12,'ynu':13}\n",
    "\n",
    "data_type='train'\n",
    "#filename,image,labelを格納するリストを作成\n",
    "#画像とラベルを返す\n",
    "filenames=[]\n",
    "images=[]\n",
    "labels=[]\n",
    "    \n",
    "    \n",
    "#条件に一致する要素のみを抽出\n",
    "#faces directoryからdirectoryが何もない以外のものとtrain dataのみを抽出\n",
    "walk=filter(lambda x:not len(x[1]),os.walk(r'D:/faces/{}/'.format(data_type.strip())) )\n",
    "    \n",
    "    \n",
    "for root,dirs,files in walk:\n",
    "    filenames+=['{}/{}'.format(root,path) for path in files if not path.startswith('.')]\n",
    "#ランダムなファイルネームを選択\n",
    "random.shuffle(filenames)\n",
    "images=[]\n",
    "for idx in range(len(filenames)):\n",
    "    file=filenames[idx]\n",
    "    image=cv2.imread(file,3)\n",
    "    image=cv2.resize(image,(32,32))\n",
    "    image=image.reshape((-1,32*32))\n",
    "    \n",
    "    images.append(image)\n",
    "\n",
    "#labelを作成\n",
    "for filename in filenames:\n",
    "    label=np.zeros(14)\n",
    "    filename=filename.split('/')[-2]\n",
    "    filename=filename[:-4]\n",
    "\n",
    "    for key,value in label_dict.items():\n",
    "        if key in filename:\n",
    "            label[value]=1\n",
    "    labels.append(list(label))\n",
    "    \n",
    "print(np.array(images).shape)\n",
    "print(np.array(labels).shape)\n",
    "\n",
    "X_train=np.array(images).reshape(-1,32,32,3)/255\n",
    "X_train=X_train.astype('float32')\n",
    "y_train=np.array(labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_shapoe: (867, 32, 32, 3)\n",
      "test_shape: (228, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "def load_data(data_type):\n",
    "    filenames=[]\n",
    "    images=[]#x_\n",
    "    labels=[]#y_\n",
    "    \n",
    "    #faces directoryからdata_typeを取り出す\n",
    "    walk=filter(lambda x: not len(x[1]),os.walk(r'D:/faces/{}'.format(data_type.strip())))\n",
    "    \n",
    "    for root,dirs,files in walk:\n",
    "        filenames+=['{}/{}'.format(root,path) for path in files if not path.startswith('.')]\n",
    "        \n",
    "        \n",
    "    #filenamesをランダムに選択\n",
    "    random.shuffle(filenames)\n",
    "    for idx in range(len(filenames)):\n",
    "        image=cv2.imread(filenames[idx],3)#chunnel3 RGB\n",
    "        image=cv2.resize(image,(32,32))\n",
    "        image=image.reshape((-1,32*32))\n",
    "        \n",
    "        images.append(image)\n",
    "        \n",
    "    for filename in filenames:\n",
    "        #labelの初期定義\n",
    "        label=np.zeros(14)\n",
    "        filename=filename.split('/')[-2]\n",
    "        filename=filename[:-4]\n",
    "        #辞書形式をイタレーション\n",
    "        for key,value in label_dict.items():\n",
    "            if key in filename:\n",
    "                label[value]=1\n",
    "        labels.append(list(label))\n",
    "        \n",
    "    images=np.array(images)\n",
    "    labels=np.array(labels)\n",
    "    return images,labels\n",
    "\n",
    "X_train,y_train=load_data('train')\n",
    "X_test,y_test=load_data('test')\n",
    "\n",
    "X_train=X_train.reshape(-1,32,32,3).astype('float32')/255\n",
    "X_test=X_test.reshape(-1,32,32,3).astype('float32')/255\n",
    "\n",
    "print('train_shapoe:',X_train.shape)\n",
    "print('test_shape:',X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cnn.py\n",
    "\n",
    "\"\"\"batch学習をする時点でX_dataは一次元配列に変換されている\"\"\"\n",
    "def batch_generator(X_train,y_train,batch_size=25,shuffle=True,random_state=None):\n",
    "    X_data=np.array(X_train)\n",
    "    y_data=np.array(y_train)\n",
    "    idx=np.arange(y_data.shape[0])\n",
    "    np.random.RandomState(random_state).shuffle(idx)\n",
    "    \n",
    "    X_data=X_data[idx]\n",
    "    y_data=y_data[idx]\n",
    "    \n",
    "    for i in range(0,X_data.shape[0],batch_size):\n",
    "        yield(X_data[i:i+batch_size,:],y_data[i:i+batch_size])\n",
    "\n",
    "\n",
    "class CNN(object):\n",
    "    \"\"\"init:batch_size バッチ学習、epochs:学習回数,learning_rate:学習率、drop_out:ドロップアウト率\n",
    "       shuffle:シャッフルするかどうか、shuffleするのでseedを固定\n",
    "       build:\n",
    "       train:\n",
    "       load:\n",
    "       save:\n",
    "       predict:\"\"\"\n",
    "    \n",
    "    def __init__(self,batch_size=25,epochs=20,learning_rate=1.0e-4,drop_out=0.5,shuffle=True,\n",
    "                random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        self.epochs=epochs\n",
    "        self.learning_rate=learning_rate\n",
    "        self.drop_out=drop_out\n",
    "        self.shuffle=shuffle\n",
    "        \n",
    "        #tensorflowのGraphの初期化\n",
    "        g=tf.Graph()\n",
    "        with g.as_default():\n",
    "            tf.set_random_seed(random_state)\n",
    "            \n",
    "            \n",
    "            #CNN構築\n",
    "            self.build()\n",
    "            \n",
    "            #変数を初期化\n",
    "            self.init_op=tf.global_variables_initializer()\n",
    "            \n",
    "            #saverの設定\n",
    "            self.saver=tf.train.Saver()\n",
    "            \n",
    "        self.sess=tf.Session(graph=g)\n",
    "        \n",
    "    def build(self):\n",
    "        tf_x=tf.placeholder(tf.float32,shape=[None,32*32*3],name='tf_x')\n",
    "        tf_y=tf.placeholder(tf.int32,shape=[None,14],name='tf_y')\n",
    "        is_train=tf.placeholder(tf.bool,shape=(),name='is_train')\n",
    "        \n",
    "        #xを四次元テンソルに変換[batch_size,h,w,channels]\n",
    "        tf_x_img=tf.reshape(tf_x,[-1,32,32,3],name='tf_x_img')\n",
    "    \n",
    "        #第一層\n",
    "        h1=tf.layers.conv2d(tf_x_img,kernel_size=(5,5),filters=32,activation=tf.nn.relu)\n",
    "        \n",
    "        #pooling\n",
    "        h1_pool=tf.layers.max_pooling2d(h1,pool_size=(2,2),strides=(2,2))\n",
    "        \n",
    "        #第二層\n",
    "        h2=tf.layers.conv2d(h1_pool,kernel_size=(5,5),filters=64,activation=tf.nn.relu)\n",
    "        \n",
    "        #pooling\n",
    "        h2_pool=tf.layers.max_pooling2d(h2,pool_size=(2,2),strides=(2,2))\n",
    "        \n",
    "        #第三層 全結合層\n",
    "        #全結合層に前の形を取得\n",
    "        input_shape=h2_pool.get_shape().as_list()\n",
    "        #最初のバイアス以外をprod\n",
    "        n_input_units=np.prod(input_shape[1:])\n",
    "        h2_pool_flat=tf.reshape(h2_pool,shape=[-1,n_input_units])\n",
    "        \n",
    "        \n",
    "        #全結合層はdenseを用いる\n",
    "        h3=tf.layers.dense(h2_pool_flat,1024,activation=tf.nn.relu)\n",
    "        \n",
    "        #dropout\n",
    "        h3_dropout=tf.layers.dropout(h3,rate=self.drop_out,training=is_train)\n",
    "        \n",
    "        #線形活性化　第四層\n",
    "        h4=tf.layers.dense(h3_dropout,14,activation=None)\n",
    "        \n",
    "        #予測辞書\n",
    "        predictions={'probabilities':tf.nn.softmax(h4,name='probabilities'),\n",
    "                    'labels':tf.cast(tf.argmax(h4,axis=1),tf.int32,name='labels')}\n",
    "        \n",
    "        #損失関数\n",
    "        cross_entropy_loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=h4,labels=tf_y),name='cross_entropy_loss')\n",
    "        \n",
    "        #最適化\n",
    "        optimizer=tf.train.AdamOptimizer(self.learning_rate)\n",
    "        optimizer=optimizer.minimize(cross_entropy_loss,name='train_op')\n",
    "        \n",
    "        #予測正解率を特定\n",
    "        correct_predictions=tf.equal(predictions['labels'],tf_y,name='correct_preds')\n",
    "        \n",
    "        accuracy=tf.reduce_mean(tf.cast(correct_predictions,tf.float32),name='accuracy')\n",
    "        \n",
    "    #学習したモデルを保存する\n",
    "    def save(self,epoch,path='./tflayers-model/'):\n",
    "        if not os.path.isdir(path):\n",
    "            os.mkdir(path)\n",
    "        print('saving_model in {}'.format(path))\n",
    "        self.saver.save(self.sess,os.path.join(path,'model.ckpt'),global_step=epoch)\n",
    "        \n",
    "    def load(self,epoch,path):\n",
    "        print('loading model from {}'.format(path))\n",
    "        \n",
    "        self.saver.restore(self.sess,os.path.join(path,'model.ckpt-{}'.format(epoch)))\n",
    "        \n",
    "    def train(self,training_set,validation_set=None,initialize=True):\n",
    "        if initialize:\n",
    "            self.sess.run(self.init_op)\n",
    "            \n",
    "        X_data=np.array(training_set[0])\n",
    "        y_data=np.array(training_set[1])\n",
    "        \n",
    "        #コスト関数を格納するリスト\n",
    "        self.training_loss=[]\n",
    "        \n",
    "        for epoch in range(1,self.epochs+1):\n",
    "            #バッチ学習のためのデータ作成\n",
    "            batch_gen=batch_generator(X_data,y_data,shuffle=self.shuffle)\n",
    "            #バッチ学習一回当たりの損失マン\n",
    "            avg_loss=0.0\n",
    "            for i,(batch_x,batch_y) in enumerate(batch_gen):\n",
    "                feed={'tf_x_img:0':batch_x,'tf_y:0':batch_y,'is_train:0':True}#dropout\n",
    "                \n",
    "                loss,_=self.sess.run(['cross_entropy_loss:0','train_op'],feed_dict=feed)\n",
    "                \n",
    "                avg_loss+=loss\n",
    "                \n",
    "            print('epoch %02d: training loss: %7.3f'%(epoch,avg_loss))\n",
    "            \n",
    "            if validation_set is not None:\n",
    "                feed={'tf_x_img:0':batch_x,'tf_y:0':batch_y,'is_train':False}\n",
    "                \n",
    "                valid_acc=self.sess.run('accuracy:0',feed_dict=feed)\n",
    "                \n",
    "                print('validation acc%7.3f'%valid_acc)\n",
    "                \n",
    "            else:\n",
    "                print()\n",
    "                \n",
    "    \n",
    "    def predict(self,X_test,return_proba=False):\n",
    "        feed={'tf_x_img:0':X_test,'is_train:0':False}#dropoutなし\n",
    "        if return_proba:\n",
    "            self.sess.run('probabilities:0',feed_dict=feed)\n",
    "            \n",
    "        else:\n",
    "            self.sess.run('labels:0',feed_dict=feed)\n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01: training loss:  84.971\n",
      "\n",
      "epoch 02: training loss:  83.386\n",
      "\n",
      "epoch 03: training loss:  82.641\n",
      "\n",
      "epoch 04: training loss:  82.145\n",
      "\n",
      "epoch 05: training loss:  82.020\n",
      "\n",
      "epoch 06: training loss:  80.863\n",
      "\n",
      "epoch 07: training loss:  79.600\n",
      "\n",
      "epoch 08: training loss:  79.192\n",
      "\n",
      "epoch 09: training loss:  78.748\n",
      "\n",
      "epoch 10: training loss:  77.702\n",
      "\n",
      "epoch 11: training loss:  76.900\n",
      "\n",
      "epoch 12: training loss:  75.405\n",
      "\n",
      "epoch 13: training loss:  75.334\n",
      "\n",
      "epoch 14: training loss:  74.820\n",
      "\n",
      "epoch 15: training loss:  73.999\n",
      "\n",
      "epoch 16: training loss:  72.418\n",
      "\n",
      "epoch 17: training loss:  73.132\n",
      "\n",
      "epoch 18: training loss:  71.658\n",
      "\n",
      "epoch 19: training loss:  71.471\n",
      "\n",
      "epoch 20: training loss:  70.672\n",
      "\n",
      "saving_model in ./tflayers-model/\n"
     ]
    }
   ],
   "source": [
    "#main関数\n",
    "def main():\n",
    "    \n",
    "    #training phase\n",
    "    cnn=CNN(random_state=123)\n",
    "    cnn.train(training_set=(X_train,y_train))\n",
    "    cnn.save(epoch=20)\n",
    "    \n",
    "    #test phase\n",
    "    del cnn\n",
    "    cnn2=CNN(random_state=123)\n",
    "    cnn2.load(epoch=20,path='./tflayers-model')\n",
    "    print('predictions\\n',cnn2.predict(X_test[:10,:]))\n",
    "    \n",
    "    preds=cnn2.predict(X_test)\n",
    "    print('test accuracy%7.3f'%(np.sum(preds=y_test)/len(preds)))\n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
